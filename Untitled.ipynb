{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "18yVGbVoPNkJFI_ZrnlYuu-UU-7Ra8lRh",
      "authorship_tag": "ABX9TyOoTjG1ahKOEJja987pEA6h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andresvir14/HWDCC_LFTK-/blob/master/Untitled.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Faa3uzLIaPeH",
        "colab_type": "text"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This notebook aims to show the pipeline that predicts Futures Sales based on C1 data, getting a score around 0.94 in the leader board (in both public and private parts).\n",
        "\n",
        "Behind what is shown in this notebook there is a lot of more work done. Nevertheless, because of the amount of hypothesis tested about the behaviour of the predictions and the ways to get a better score, I going to show only the key points that worked best.\n",
        "\n",
        "The notebook is organized as follows:\n",
        "\n",
        "0. Initial steps (getting the libraries, functions and data) \n",
        "1. Basic pre-processing of data\n",
        "2. Mean encodings\n",
        "3. Lagged columns\n",
        "4. Split of data\n",
        "5. Model fitting through XGboost\n",
        "6. Getting the file for submission\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7scGlPBevlG",
        "colab_type": "text"
      },
      "source": [
        "#0. Initial steps\n",
        "\n",
        "Loading of libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xr-u9bHh6qbA",
        "colab_type": "code",
        "outputId": "92c26192-d6aa-4136-ba1d-6181d5928407",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Import libraries\n",
        "# Basic libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Preprocesing and feature extraction\n",
        "from sklearn.model_selection import KFold\n",
        "from itertools import product\n",
        "from sklearn import preprocessing\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Modeling\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from xgboost import XGBRegressor\n",
        "from xgboost import plot_importance\n",
        "\n",
        "# Ploting\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "import seaborn as sns\n",
        "\n",
        "# To deal with downloaded files\n",
        "from google.colab import files\n",
        "\n",
        "# Others\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPakU6suZ_e-",
        "colab_type": "text"
      },
      "source": [
        "Definition of functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWhrGXi38Jom",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Functions used\n",
        "\n",
        "# Downcast variables \n",
        "def downcast(df):\n",
        "  '''\n",
        "    Change columns types from 64 to 32 bits\n",
        "  '''\n",
        "  float64columns = df.select_dtypes(['float64']).columns.tolist()\n",
        "  int64columns = df.select_dtypes(['int64']).columns.tolist()\n",
        "  df[float64columns] = df[float64columns].astype('float32')\n",
        "  df[int64columns] = df[int64columns].astype('int32')\n",
        "  return df\n",
        "\n",
        "\n",
        "# Function that creates the lags of given columns (taken from he notebook of https://www.kaggle.com/dlarionov/feature-engineering-xgboost)\n",
        "def lag_feature(df, lags, col):\n",
        "    tmp = df[['date_block_num','shop_id','item_id', col]]\n",
        "    for i in lags:\n",
        "        shifted = tmp.copy()\n",
        "        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+ str(i)]\n",
        "        shifted['date_block_num'] += i\n",
        "        shifted[col+'_lag_'+ str(i)] = shifted[col+'_lag_'+ str(i)].astype('float16')\n",
        "        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n",
        "    return df    \n",
        "\n",
        "# Applying target (meen) encodings as https://www.kaggle.com/ogrellier/python-target-encoding-for-categorical-features\n",
        "def add_noise(series, noise_level):\n",
        "    return series * (1 + noise_level * np.random.randn(len(series)))\n",
        "\n",
        "def target_encode(trn_series=None, \n",
        "                  tst_series=None, \n",
        "                  target=None, \n",
        "                  min_samples_leaf=1, \n",
        "                  smoothing=1,\n",
        "                  noise_level=0):\n",
        "    \"\"\"\n",
        "    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n",
        "    https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf\n",
        "    trn_series : training categorical feature as a pd.Series\n",
        "    tst_series : test categorical feature as a pd.Series\n",
        "    target : target data as a pd.Series\n",
        "    min_samples_leaf (int) : minimum samples to take category average into account\n",
        "    smoothing (int) : smoothing effect to balance categorical average vs prior  \n",
        "    \"\"\" \n",
        "    assert len(trn_series) == len(target)\n",
        "    assert trn_series.name == tst_series.name\n",
        "    temp = pd.concat([trn_series, target], axis=1)\n",
        "    # Compute target mean \n",
        "    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n",
        "    # Compute smoothing\n",
        "    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n",
        "    # Apply average function to all target data\n",
        "    prior = target.mean()\n",
        "    # The bigger the count the less full_avg is taken into account\n",
        "    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n",
        "    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n",
        "    # Apply averages to trn and tst series\n",
        "    ft_trn_series = pd.merge(\n",
        "        trn_series.to_frame(trn_series.name),\n",
        "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
        "        on=trn_series.name,\n",
        "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
        "    # pd.merge does not keep the index so restore it\n",
        "    ft_trn_series.index = trn_series.index \n",
        "    ft_tst_series = pd.merge(\n",
        "        tst_series.to_frame(tst_series.name),\n",
        "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
        "        on=tst_series.name,\n",
        "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
        "    # pd.merge does not keep the index so restore it\n",
        "    ft_tst_series.index = tst_series.index\n",
        "    return pd.concat([add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)], axis = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9009qv2ZfCi8",
        "colab_type": "text"
      },
      "source": [
        "Loading of data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVmnEFrM66HL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load de the data\n",
        "transactions    = pd.read_csv('/content/drive/My Drive/DataScience/data/competitive-data-science-predict-future-sales/sales_train.csv')\n",
        "items           = pd.read_csv('/content/drive/My Drive/DataScience/data/competitive-data-science-predict-future-sales/items.csv')\n",
        "item_categories = pd.read_csv('/content/drive/My Drive/DataScience/data/competitive-data-science-predict-future-sales/item_categories.csv')\n",
        "shops           = pd.read_csv('/content/drive/My Drive/DataScience/data/competitive-data-science-predict-future-sales/shops.csv')\n",
        "transactions_test    = pd.read_csv('/content/drive/My Drive/DataScience/data/competitive-data-science-predict-future-sales/test.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQFnzQHzfSH6",
        "colab_type": "text"
      },
      "source": [
        "# 1. Basic pre-processing of data\n",
        "\n",
        "In firts place, I did the data aggregation thanks to the use of part of the code from week four of the course."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEKnqvug6-gR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "index_cols = ['shop_id', 'item_id', 'date_block_num']\n",
        "\n",
        "# For every month we create a grid from all shops/items combinations from that month\n",
        "grid = [] \n",
        "for block_num in transactions['date_block_num'].unique():\n",
        "    cur_shops = transactions[transactions['date_block_num']==block_num]['shop_id'].unique()\n",
        "    cur_items = transactions[transactions['date_block_num']==block_num]['item_id'].unique()\n",
        "    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])), dtype = 'int16'))\n",
        "\n",
        "# Turn the grid into pandas dataframe\n",
        "grid = pd.DataFrame(np.vstack(grid), columns = index_cols, dtype = np.int32)\n",
        "\n",
        "# Get aggregated values of sales by shop_id, item_id\n",
        "gb = transactions.groupby(index_cols).agg({'item_cnt_day':'sum'})\n",
        "gb.columns = ['target']\n",
        "gb.reset_index(inplace = True)\n",
        "\n",
        "# Join aggregated data to the grid and sort the data\n",
        "train = pd.merge(grid, gb, how='left',on=index_cols).fillna(0)\n",
        "train['target'] = train['target'].clip(0,20)\n",
        "train.sort_values(['date_block_num','shop_id','item_id'],inplace=True)\n",
        "\n",
        "del grid, gb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Coggv1P98j5W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add items and shops to training data\n",
        "train = pd.merge(train, items, on = ['item_id']) # Join item_name and item category\n",
        "train = pd.merge(train, shops, on = ['shop_id']) # Join shop_name"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8lsFtHT84V9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set ID as index and join shops and items to test data\n",
        "test = transactions_test.copy()\n",
        "test.drop('ID', axis = 1, inplace = True)\n",
        "test['date_block_num'] = 34\n",
        "\n",
        "test = pd.merge(test, items, on = ['item_id']) # Join item_name and item category\n",
        "test = pd.merge(test, shops, on = ['shop_id']) # Join shop_name\n",
        "\n",
        "del shops, items, item_categories"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdNd2qitDmV7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Concatenating train and test sets\n",
        "train['from'] = 0\n",
        "test['from'] = 1\n",
        "data = pd.concat([train, test], axis = 0)\n",
        "data.reset_index(inplace=True)\n",
        "\n",
        "del train, test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imyHSRvpf27U",
        "colab_type": "text"
      },
      "source": [
        "# 3. Mean encondings\n",
        "\n",
        "Mean encondings are a powerful tool to handle with high cardenality categorical features. Althougth, in the course were given several methods for compute regularized mean encondings( and I tryed Kfold, smothing and LOO, based on the code given in the course). I decided to applied and slightly change the code found in this notebook https://www.kaggle.com/ogrellier/python-target-encoding-for-categorical-features. \n",
        "\n",
        "The computations of mean (target) encodings were done over the iten_target and pairs of columns as date_block-item_id and shop_id-item_id. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XW5ef4xExuS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "aa5b653d-23ee-4762-e853-883aa22afe4b"
      },
      "source": [
        "#data['date_block_num_target_ME'] = target_encode(data[data['from'] == 0][\"date_block_num\"], \n",
        "#                         data[data['from'] == 1][\"date_block_num\"], \n",
        "#                         target=data[data['from'] == 0].target, \n",
        "#                         min_samples_leaf=100,\n",
        "#                         smoothing=10,\n",
        "#                         noise_level=0.01)\n",
        "\n",
        "#data['shop_id_target_ME'] = target_encode(data[data['from'] == 0][\"shop_id\"], \n",
        "#                         data[data['from'] == 1][\"shop_id\"], \n",
        "#                         target=data[data['from'] == 0].target, \n",
        "#                         min_samples_leaf=100,\n",
        "#                         smoothing=10,\n",
        "#                         noise_level=0.01)\n",
        "\n",
        "#data['item_category_id_target_ME'] = target_encode(data[data['from'] == 0][\"item_category_id\"], \n",
        "#                         data[data['from'] == 1][\"item_category_id\"], \n",
        "#                         target=data[data['from'] == 0].target, \n",
        "#                         min_samples_leaf=100,\n",
        "#                         smoothing=10,\n",
        "#                         noise_level=0.01)\n",
        "\n",
        "data['item_id_target_ME'] = target_encode(data[data['from'] == 0][\"item_id\"], \n",
        "                            data[data['from'] == 1][\"item_id\"], \n",
        "                            target=data[data['from'] == 0].target, \n",
        "                            min_samples_leaf=100,\n",
        "                            smoothing=10,\n",
        "                            noise_level=0.01)\n",
        "\n",
        "\n",
        "data['date_item'] = data['date_block_num'].astype(str) + data['item_id'].astype(str)\n",
        "#data['date_shop'] = data['date_block_num'].astype(str) + data['shop_id'].astype(str)\n",
        "#data['date_cat'] = data['date_block_num'].astype(str) + data['item_category_id'].astype(str)\n",
        "#data['shop_cat'] = data['shop_id'].astype(str) + data['item_category_id'].astype(str)\n",
        "data['shop_item'] = data['shop_id'].astype(str) + data['item_id'].astype(str)                         \n",
        "\n",
        "data['date_item_target_ME'] = target_encode(data[data['from'] == 0][\"date_item\"], \n",
        "                            data[data['from'] == 1][\"date_item\"], \n",
        "                            target=data[data['from'] == 0].target, \n",
        "                            min_samples_leaf=100,\n",
        "                            smoothing=10,\n",
        "                            noise_level=0.01)\n",
        "\n",
        "#data['date_shop_target_ME'] = target_encode(data[data['from'] == 0][\"date_shop\"], \n",
        "#                         data[data['from'] == 1][\"date_shop\"], \n",
        "#                         target=data[data['from'] == 0].target, \n",
        "#                         min_samples_leaf=100,\n",
        "#                         smoothing=10,\n",
        "#                         noise_level=0.01)\n",
        "\n",
        "#data['date_cat_target_ME'] = target_encode(data[data['from'] == 0][\"date_cat\"], \n",
        "#                         data[data['from'] == 1][\"date_cat\"], \n",
        "#                         target=data[data['from'] == 0].target, \n",
        "#                         min_samples_leaf=100,\n",
        "#                         smoothing=10,\n",
        "#                         noise_level=0.01)\n",
        "\n",
        "#data['shop_cat_target_ME'] = target_encode(data[data['from'] == 0][\"shop_cat\"], \n",
        "#                         data[data['from'] == 1][\"shop_cat\"], \n",
        "#                         target=data[data['from'] == 0].target, \n",
        "#                         min_samples_leaf=100,\n",
        "#                         smoothing=10,\n",
        "#                         noise_level=0.01)\n",
        "\n",
        "\n",
        "data['shop_item_target_ME'] = target_encode(data[data['from'] == 0][\"shop_item\"], \n",
        "                         data[data['from'] == 1][\"shop_item\"], \n",
        "                         target=data[data['from'] == 0].target, \n",
        "                         min_samples_leaf=100,\n",
        "                         smoothing=10,\n",
        "                         noise_level=0.01)\n",
        "\n",
        "#data.drop(['date_item', 'date_shop', 'date_cat', 'shop_cat', 'shop_item'], axis = 1, inplace = True)\n",
        "data.drop(['date_item', 'shop_item'], axis = 1, inplace = True)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-d7ff29e80624>\"\u001b[0;36m, line \u001b[0;32m41\u001b[0m\n\u001b[0;31m    target=data[data['from'] == 0].target,\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xdhw0-ocFhJI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.isnull().sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvZu80Ox6wKp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koOh6nvY9QeI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lagged values\n",
        "ts = time.time()\n",
        "\n",
        "data = lag_feature(data, [1, 2, 3, 4, 5, 12], 'target')\n",
        "data = lag_feature(data, [1, 2, 3, 4, 5, 12], 'date_block_num_target_ME')\n",
        "data = lag_feature(data, [1, 2, 3, 4, 5, 12], 'shop_id_target_ME')\n",
        "data = lag_feature(data, [1, 2, 3, 4, 5, 12], 'item_category_id_target_ME')\n",
        "data = lag_feature(data, [1, 2, 3, 4, 5, 12], 'item_id_target_ME')\n",
        "data = lag_feature(data, [1, 2, 3, 4, 5, 12], 'date_item_target_ME')\n",
        "data = lag_feature(data, [1, 2, 3, 4, 5, 12], 'date_shop_target_ME')\n",
        "data = lag_feature(data, [1, 2, 3, 4, 5, 12], 'date_cat_target_ME')\n",
        "data = lag_feature(data, [1, 2, 3, 4, 5, 12], 'shop_cat_target_ME')\n",
        "data = lag_feature(data, [1, 2, 3, 4, 5, 12], 'shop_item_target_ME')\n",
        "\n",
        "data = downcast(data)\n",
        "\n",
        "time.time() - ts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqegq3ir_og-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Drop level 0 lagged features\n",
        "\n",
        "#data.drop(['date_block_num_target_ME', 'shop_id_target_ME', 'item_category_id_target_ME', 'item_id_target_ME', 'date_item_target_ME', 'date_shop_target_ME', 'date_cat_target_ME', 'shop_cat_target_ME', 'shop_item_target_ME'], axis = 1, inplace = True) # Drop object columns\n",
        "data = downcast(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PPCCLD79k6z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.fillna(0, inplace=True)\n",
        "data.drop(['item_name', 'shop_name'], axis = 1, inplace = True) # Drop object columns\n",
        "\n",
        "X_train = data[(data.date_block_num > 11) & (data.date_block_num < 33)].drop(['target'], axis=1)\n",
        "Y_train = data[(data.date_block_num > 11) & (data.date_block_num < 33)]['target']\n",
        "X_valid = data[data.date_block_num == 33].drop(['target'], axis=1)\n",
        "Y_valid = data[data.date_block_num == 33]['target']\n",
        "X_test = data[data.date_block_num == 34].drop(['target'], axis=1)\n",
        "\n",
        "# del data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-aUwySDxTkE",
        "colab_type": "text"
      },
      "source": [
        "## First level models\n",
        "\n",
        "linear regression and XGB model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2n0rV2l7-TUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ts = time.time()\n",
        "\n",
        "model = XGBRegressor(\n",
        "    tree_method = \"gpu_hist\",\n",
        "    max_depth = 6, # Tree related parameter: determines how deeply each tree is allowed to grow during any boosting round\n",
        "    min_child_weight = 600, # \n",
        "    colsample_bytree = 0.8, # percentage of features used per tree. High value can lead to overfitting\n",
        "    n_estimators = 500, # number of trees you want to build.\n",
        "    subsample = 0.80, # Boosting parameter: percentage of samples used per tree. Low value can lead to underfitting\n",
        "    eta=0.3, #  Boosting parameter\n",
        "    seed=123)\n",
        "\n",
        "model.fit(\n",
        "    X_train, \n",
        "    Y_train, \n",
        "    eval_metric=\"rmse\", \n",
        "    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n",
        "    verbose=True, \n",
        "    early_stopping_rounds = 10)\n",
        "\n",
        "time.time() - ts\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkUnAPYa-qfc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feat_importances = pd.Series(model.feature_importances_, index=X_train.columns)\n",
        "feat_importances.nlargest(15).plot(kind='barh')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gj3WoGm-sd7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_test = model.predict(X_test).clip(0, 20)\n",
        "testdata = X_test.copy()\n",
        "testdata['Y_test'] = Y_test\n",
        "\n",
        "# Add target variable to transactions_test and leave only ID and target\n",
        "transactions_test.reset_index(inplace = True)\n",
        "submition = pd.merge(transactions_test, testdata[['shop_id', 'item_id', 'Y_test']], on = ['shop_id', 'item_id'])\n",
        "submition = submition[['ID', 'Y_test']]\n",
        "submition.columns = ['ID', 'item_cnt_month']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UU227a7MRds3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fit in training + valida data\n",
        "\n",
        "X_train_c = data[(data.date_block_num > 11) & (data.date_block_num < 34)].drop(['target'], axis=1)\n",
        "Y_train_c = data[(data.date_block_num > 11) & (data.date_block_num < 34)]['target']\n",
        "X_test = data[data.date_block_num == 34].drop(['target'], axis=1)\n",
        "\n",
        "ts = time.time()\n",
        "model.fit(\n",
        "    X_train_c, \n",
        "    Y_train_c, \n",
        "    eval_metric=\"rmse\")\n",
        "time.time() - ts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOhagOWORxAu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_test = model.predict(X_test).clip(0, 20)\n",
        "testdata = X_test.copy()\n",
        "testdata['Y_test'] = Y_test\n",
        "\n",
        "# Add target variable to transactions_test and leave only ID and target\n",
        "#transactions_test.reset_index(inplace = True)\n",
        "submition = pd.merge(transactions_test, testdata[['shop_id', 'item_id', 'Y_test']], on = ['shop_id', 'item_id'])\n",
        "submition = submition[['ID', 'Y_test']]\n",
        "submition.columns = ['ID', 'item_cnt_month']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaaBIjeh-s48",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Export results\n",
        "submition.to_csv('V21_SmoothKag_All11_ShopItemME34L0.csv', index=False)\n",
        "files.download('V21_SmoothKag_All11_ShopItemME34L0.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2WPL_ByIZmg",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}